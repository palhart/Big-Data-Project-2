version: "3.9"

networks:
  hadoop-net:
    driver: bridge

services:
  # Distributed storage
  hdfs-namenode:
    container_name: "hdfs-namenode"
    image: "apache/hadoop:3"
    hostname: "hdfs-namenode"
    command: ["hdfs", "namenode"]
    ports:
      - "8020:8020"
      - "9870:9870"
    env_file:
      - ./hadoop-config/config.env
    environment:
      ENSURE_NAMENODE_DIR: "/tmp/hadoop-root/dfs/name"
    networks:
      - hadoop-net

  hdfs-datanode:
    depends_on:
      hdfs-namenode:
        condition: service_started
    container_name: "hdfs-datanode"
    image: "apache/hadoop:3"
    hostname: "hdfs-datanode"
    command: ["hdfs", "datanode"]
    ports:
      - "9864:9864"
    env_file:
      - ./hadoop-config/config

  # Data Processing with Spark
  data-processing:
    build:
      context: ./data_processing
      dockerfile: Dockerfile
    container_name: "data-processing"
    depends_on:
      hdfs-namenode:
        condition: service_started
    environment:
      - SPARK_MASTER=local[*]
      - HADOOP_CONF_DIR=/opt/hadoop/etc/hadoop
    command: ["spark-submit", "--master", "local[*]", "/app/data_pipeline.py"]
    volumes:
      - ./data_processing:/app/data_processing/
      - ./hadoop-config:/opt/hadoop/etc/hadoop
    networks:
      - hadoop_network

networks:
  hadoop_network:
    driver: bridge